batch_size: &batch_size 64
pretrained_model_name: distilbert-base-uncased
finetuned_model_name: &finetuned_model_name finetuned-distilbert-base-uncased

training_arguments:
  output_dir: *finetuned_model_name
  num_train_epochs: 2
  learning_rate: 2.0e-5
  per_device_train_batch_size: *batch_size
  per_device_eval_batch_size: *batch_size
  weight_decay: 0.01
  evaluation_strategy: epoch
  disable_tqdm: false
  logging_steps: ~
  push_to_hub: false
  log_level: error

tokenize_params:
  padding: true
  truncation: true
